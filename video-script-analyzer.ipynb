{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport openai\nimport langchain as lc\nfrom langchain.document_loaders import SRTLoader\nfrom dotenv import load_dotenv\nimport srt\nimport codecs\nimport tiktoken\nfrom IPython.display import clear_output\n\n# Load environment variables\nload_dotenv()\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nopenai_api_key = user_secrets.get_secret(\"openai\")\n\nif not openai_api_key:\n    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n\nopenai.api_key = openai_api_key\n\nGPT_MODEL_NAME = \"gpt-4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_tokens_from_string(string: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\ndef get_completion(prompt, model_name: str):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model= model_name,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\ndef write_to_file(content: str, file_path: str):\n    \"\"\"Write content to a file.\"\"\"\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(content)\n\ndef read_srt_file(file_path: str):\n    \"\"\"Read and parse an SRT file.\"\"\"\n    with codecs.open(file_path, 'r', encoding='utf-8-sig') as f:\n        return list(srt.parse(f.read()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Audio download from Youtube","metadata":{}},{"cell_type":"code","source":"pip install pytube\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytube import YouTube\nfrom pydub import AudioSegment\nimport subprocess\nimport os\n\n# Improved: Added error handling and streamlined audio format\n\ndef download_video(youtube_link: str, output_filename: str = \"stream\"):\n    \"\"\"Downloads the video stream from a YouTube video.\"\"\"\n    try:\n        youtube = YouTube(youtube_link)\n        # Selecting the best audio stream\n        video_stream = youtube.streams.first()\n        if not video_stream:\n            raise Exception(\"No video stream found in the YouTube video.\")\n\n        # Downloading and saving the stream\n        video_stream.download(filename=f\"{output_filename}.mp4\")\n\n    except Exception as e:\n        print(f\"Error downloading video: {e}\")\n\n# YouTube video URL\nurl = 'https://www.youtube.com/watch?v=xlRk4vlqLm0&t=87s'\n\ndownload_video(url)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract Audio from Video","metadata":{}},{"cell_type":"code","source":"video_file_name = \"stream\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from moviepy.editor import VideoFileClip\n\ndef extract_audio(video_file: str, audio_file: str):\n    \"\"\"Extracts audio from a video file and saves it as an audio file.\"\"\"\n    try:\n        with VideoFileClip(video_file) as video:\n            audio = video.audio\n            audio.write_audiofile(audio_file)\n    except Exception as e:\n        print(f\"Error extracting audio: {e}\")\n\n# Usage\nextract_audio(f\"{video_file_name}.mp4\", \"output_audio.mp3\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Whisper Transcription","metadata":{}},{"cell_type":"code","source":"import torch\n\nis_cuda_available = torch.cuda.is_available()\nprint(f\"CUDA Available: {is_cuda_available}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install openai-whisper==20230117\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import whisper\nfrom whisper.utils import write_srt\nfrom pydub import AudioSegment\n\nmodel = whisper.load_model(\"medium\")\naudio = \"output_audio.mp3\"\nresult = model.transcribe(audio)\noutput_directory = \"./\"\n\n\n# Save as an SRT file\ntry:\n    srt_writer = write_srt(\"srt\", output_directory)\n    srt_writer(result, audio)\n    print(f\"SRT file saved.\")\nexcept Exception as e:\n    print(f\"Error saving SRT file: {e}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T13:44:22.347726Z","iopub.execute_input":"2023-12-18T13:44:22.348653Z","iopub.status.idle":"2023-12-18T13:44:59.368418Z","shell.execute_reply.started":"2023-12-18T13:44:22.348614Z","shell.execute_reply":"2023-12-18T13:44:59.367127Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"100%|█████████████████████████████████████| 1.42G/1.42G [00:15<00:00, 96.1MiB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_audio.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save as an SRT file\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/whisper/transcribe.py:93\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, **decode_options)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetecting language using up to the first 30 seconds. Use `--language` to specify the language\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m---> 93\u001b[0m _, probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(probs, key\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/whisper/decoding.py:35\u001b[0m, in \u001b[0;36mdetect_language\u001b[0;34m(model, mel, tokenizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(model\u001b[38;5;241m.\u001b[39mis_multilingual)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mlanguage_token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39msot_sequence:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have language tokens so it can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt perform lang id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m single \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single:\n","\u001b[0;31mValueError\u001b[0m: This model doesn't have language tokens so it can't perform lang id"],"ename":"ValueError","evalue":"This model doesn't have language tokens so it can't perform lang id","output_type":"error"}]},{"cell_type":"markdown","source":"### Creating video chapters","metadata":{}},{"cell_type":"code","source":"def process_subtitles(subtitles, model_name: str):\n    \"\"\"Process subtitles and split them into chapters.\"\"\"\n    result, chunk = \"\", \"\"\n    for sub in subtitles:\n        chunk += f\"{sub.start}>{sub.end.total_seconds()}\\n{sub.content}\"\n        if num_tokens_from_string(chunk) > 7000:\n            result += split_into_chapters(chunk, model_name)\n            chunk = \"\"        \n    if chunk:\n        result += split_into_chapters(chunk, model_name)\n    return result\n\ndef split_into_chapters(chunk: str, model_name: str):\n    \"\"\"Split the transcript chunk into chapters.\"\"\"\n    prompt = (\"Below is a part of a video transcript. You need to split the video \"\n              \"into five topic chapters. The chapters will be used to navigate in the \"\n              \"larger video timeline to let watchers switch between topics. Read the \"\n              \"entire transcript. Once done reading, split it into chapters. Provide \"\n              \"the list of chapters in this format [HH:MM:SS Chapter Name]. Put each \"\n              \"chapter in a separate line in plain text. Match the transcript language \"\n              \"in the output.\\n\\n\" + chunk)\n    return get_completion(prompt, model_name)\n\n\nsubtitles = read_srt_file(\"output_audio.srt\")\nfinal_output = process_subtitles(subtitles, GPT_MODEL_NAME)\nwrite_to_file(final_output, 'chapters-iac.txt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating video summary","metadata":{}},{"cell_type":"code","source":"def create_summary(transcript: str, model_name: str):\n    \"\"\"Create a summary of the given transcript.\"\"\"\n    prompt = (\"Below is a video transcript. Your goal is to summarize the \"\n              \"entire video. You need to create the shortest summary as \"\n              \"possible that will help a reader understand the information given in the video.\\n\\n\" + transcript)\n    return get_completion(prompt, model_name)\n\nsubtitles = read_srt_file(\"output_audio.srt\")\ntranscript_text = \"\\n\\n\".join([f\"{sub.content}\" for sub in subtitles])\nfinal_output = create_summary(transcript_text, GPT_MODEL_NAME)\nwrite_to_file(final_output, 'summaries.txt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Title Creator","metadata":{}},{"cell_type":"code","source":"def process_srt_file(file_path: str, model_name: str):\n    \"\"\"Process an SRT file to generate Turkish title alternatives and a summary.\"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n\n    prompt = (\"Below is a series of summaries created out of different sections \"\n              \"of a video recording. The video is published on YouTube. Provide \"\n              \"10 Turkish title alternatives and a single Turkish summary for the \"\n              \"video. Both title and summary should be inviting and helpful to \"\n              \"watchers.\\n\\n\" + content)\n    \n    return get_completion(prompt, model_name)\n\nfinal_output = process_srt_file(\"summaries.txt\", GPT_MODEL_NAME)\nwith open('title-description.txt', 'w', encoding='utf-8') as f:\n    f.write(final_output)","metadata":{},"execution_count":null,"outputs":[]}]}